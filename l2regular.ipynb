{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "l2regular.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/brucecmd/learn_gluon/blob/master/l2regular.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "OqbP8i1HML8M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from mxnet import nd, autograd\n",
        "from mxnet.gluon import data as gdata\n",
        "from mxnet import gluon\n",
        "from mxnet.gluon import loss as gloss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HXfQhbE6NByJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare data and data iter\n",
        "num_train = 20\n",
        "num_test = 100\n",
        "num_inputs = 200\n",
        "batch_size = 1\n",
        "x = nd.random_normal(0,1,shape=(num_train+num_test, num_inputs))\n",
        "true_w = nd.ones(shape=(num_inputs,1)) * 0.01\n",
        "true_b = 2.8\n",
        "y = nd.dot(x, true_w) + true_b\n",
        "y += nd.random_normal(0,0.01,shape=y.shape)\n",
        "feature_train, feature_test = x[:num_train,:], x[num_train:,:]\n",
        "label_train, label_test = y[:num_train], y[num_train:]\n",
        "\n",
        "train_iter = gdata.DataLoader(gdata.ArrayDataset(feature_train, label_train), batch_size, shuffle=True)\n",
        "test_iter = gdata.DataLoader(gdata.ArrayDataset(feature_test, label_test), batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rOOAl1twal-B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "即使num_train很大，num_test很小的时候，也有可能会出问题，越训练loss越大。\n",
        "这时候要看一下batch_size是多少，如果batch_size为1的时候，lr为0.01的时候表现不好。\n",
        "如果batch_size调成10的话，情况会有很大好转。\n",
        "经过参数调整实验，发现这种情况出现的原因是lr可能有点大了。因为batch_size为10的时候，sgd的时候也要把这个batch_size除一下，这个lr就会变小。\n",
        "如果直接调lr为0.001，效果也会有很大的改善。"
      ]
    },
    {
      "metadata": {
        "id": "AveyVapWPHp0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define w and b\n",
        "w = nd.random_normal(0,1,shape=(num_inputs, 1))\n",
        "b = nd.zeros(shape=(1,))\n",
        "params = [w,b]\n",
        "for p in params:\n",
        "    p.attach_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tcBJ_QSdUUj3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define net\n",
        "def net(x):\n",
        "    return nd.dot(x,w) + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmR9pgkLUgwn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define loss function \n",
        "#def loss_func(y_hat, y):\n",
        "#    return (y_hat - y.reshape(y_hat.shape))**2 / 2\n",
        "loss_func = gloss.L2Loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nf8F-9ZTVBVI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the trainer(sgd)\n",
        "def sgd(params, batch_size, lr):\n",
        "    for p in params:\n",
        "        p[:] -= p.grad * lr / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "egIurYF5Vkmq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define l2 penalty function\n",
        "def l2_penalty(w):\n",
        "    return (w**2).sum() / 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kCUYCwgvVwLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ca6684d3-6b53-474e-d4ab-9ffb4be6b224"
      },
      "cell_type": "code",
      "source": [
        "# train\n",
        "epochs = 10\n",
        "lr = 0.001\n",
        "batch_size = 1\n",
        "lambd = 0.1\n",
        "for i in range(epochs):\n",
        "    for data, label in train_iter:\n",
        "        with autograd.record():\n",
        "            y_hat = net(data)\n",
        "            #l = loss_func(y_hat, label) + lambd * l2_penalty(w)\n",
        "            l = loss_func(y_hat, label)\n",
        "        l.backward()\n",
        "        sgd(params, batch_size, lr)\n",
        "    train_loss_value = loss_func(net(feature_train), label_train).mean().asscalar()\n",
        "    test_loss_value = loss_func(net(feature_test), label_test).mean().asscalar()\n",
        "    print('epoch[%d], train loss value[%f], test loss value[%f]'%(i, train_loss_value, test_loss_value))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch[0], train loss value[25.414448], test loss value[108.884552]\n",
            "epoch[1], train loss value[16.409306], test loss value[108.569626]\n",
            "epoch[2], train loss value[10.836321], test loss value[108.380280]\n",
            "epoch[3], train loss value[7.297836], test loss value[108.287041]\n",
            "epoch[4], train loss value[4.986907], test loss value[108.238487]\n",
            "epoch[5], train loss value[3.482398], test loss value[108.225052]\n",
            "epoch[6], train loss value[2.469655], test loss value[108.217209]\n",
            "epoch[7], train loss value[1.776655], test loss value[108.204277]\n",
            "epoch[8], train loss value[1.290132], test loss value[108.203407]\n",
            "epoch[9], train loss value[0.948414], test loss value[108.199142]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}